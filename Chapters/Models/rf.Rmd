\subsection{Random Forest Models}

The Random Forest [RF] algoritm is also quite popular and can be used for regression or classification. They are known for above average results since they encompass all the benefits of decision trees, and are not prone to over-fitting. 

For classification, a Random Forest will grow many decision trees. Each tree provides a classification which is considered a vote. The alogrithm then chooses the classification that has the most votes from all the trees in the forest.

There is some debate about the need for cross validation with Random Forests since each tree is grown with a list of randomly selected samples. This process leaves out approximately $\frac{1}{3}$ of the samples for testing the grown tree. Because the samples are randomly selected, each tree has its own learning and out-of-bag sample set, making LOOCV somewhat redundant. However, it is technically not incorrect to use cross validation with Random Forests since cross validation is one way to ensure that all data is used during the training or testing phase.

```{r rf_model, echo = FALSE}

# Create a data frame to hold the results
rf_results <- data.frame(Model = character()
                          ,Data = character()
                          ,Accuracy = numeric()
                          ,Sensitivity = numeric()
                          ,Specificity = numeric()
                          ,Precision = numeric()
                          ,Kappa = numeric()
                          ,F1 = numeric()
                          ,ROC = numeric())
```

```{r rf_models, echo = FALSE}

# Enable parallel processing and reserve resources
#cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
#registerDoParallel(cluster)

# Set up training conditions - must use LOOCV
fitControl <- trainControl(method = "LOOCV"
                           ,classProbs = TRUE 
                           ,summaryFunction = twoClassSummary
                           ,savePredictions = 'final'
                           ,allowParallel = TRUE)

# Full dat set
#set.seed(1234)
#rf.model.ud <- caret::train(Label ~.
#            ,data = use_data
#            ,method = 'rf'
#            ,trControl = fitControl
#            ,metric = "ROC")

rf.model.ud <- readRDS('../../Models/rf_use_data.rds')

rf.cm.ud <- caret::confusionMatrix(rf.model.ud$pred$pred, rf.model.ud$pred$obs)

# Get the performance metrics from the model and save for comparison
performance <- getTrainPerf(rf.model.ud)
rf_results <- rbind(rf_results
                        ,data.frame(Model = 'Random Forest'
                        ,Data = 'Full'
                        ,Accuracy = rf.cm.ud$overall[1]
                        ,Sensitivity = performance[,2]
                        ,Specificity = performance[,3]
                        ,Precision = rf.cm.ud$byClass[5]
                        ,Kappa = rf.cm.ud$overall[2]
                        ,F1 = rf.cm.ud$byClass[7]
                        ,ROC = performance[,1]))

# RFE selection data set
#set.seed(1234)
#rf.model.md <- caret::train(Label ~.
#                            ,data = model_data
#                            ,method = 'rf'
#                            ,trControl = fitControl
#                            ,metric = "ROC")

rf.model.md <- readRDS('../../Models/rf_model_data.rds')

rf.cm.md <- caret::confusionMatrix(rf.model.md$pred$pred, rf.model.md$pred$obs)

# Get the performance metrics from the model and save for comparison
performance <- getTrainPerf(rf.model.md)
rf_results <- rbind(rf_results
                        ,data.frame(Model = 'Random Forest'
                        ,Data = 'RFE Selection'
                        ,Accuracy = rf.cm.md$overall[1]
                        ,Sensitivity = performance[,2]
                        ,Specificity = performance[,3]
                        ,Precision = rf.cm.md$byClass[5]
                        ,Kappa = rf.cm.md$overall[2]
                        ,F1 = rf.cm.md$byClass[7]
                        ,ROC = performance[,1]))

# Low correlation data set
#set.seed(1234)
#rf.model.lc <- caret::train(Label ~.
#                            ,data = use_data_lc
#                            ,method = 'rf'
#                            ,trControl = fitControl
#                            ,metric = "ROC")

rf.model.lc <- readRDS('../../Models/rf_use_data_lc.rds')

rf.cm.lc <- caret::confusionMatrix(rf.model.lc$pred$pred, rf.model.lc$pred$obs)

# Get the performance metrics from the model and save for comparison
performance <- getTrainPerf(rf.model.lc)
rf_results <- rbind(rf_results
                        ,data.frame(Model = 'Random Forest'
                        ,Data = 'Low Correlation'
                        ,Accuracy = rf.cm.lc$overall[1]
                        ,Sensitivity = performance[,2]
                        ,Specificity = performance[,3]
                        ,Precision = rf.cm.lc$byClass[5]
                        ,Kappa = rf.cm.lc$overall[2]
                        ,F1 = rf.cm.lc$byClass[7]
                        ,ROC = performance[,1]))

rownames(rf_results) <- NULL

# Disable parallel processing and release resources
#stopCluster(cluster)
#registerDoSEQ()

```

As seen in __Table 18__, the model using the trimmed data set from the 'rfe' function is the best of the three, and the best model of all so far. The model has an accuracy of `r round(rf.cm.md$overall[1] * 100, 2)`%. 

```{r rf_results, echo = FALSE}

kable(rf_results, caption = "Random Forest Model Results", booktabs = TRUE) %>% 
        kable_styling(font_size = 8, latex_options = c("striped", "hold_position"))

```

\FloatBarrier

__Figure 15__ provides a Variable Importance plot. This plot shows the variables that were most impactful in the model. One disadvantage of Random Forests, as well as many machine learning alogrithms, is that they operate as a 'black box' so it is difficult to understand how the model is making predictions.

The variables are listed from the top down in order of importance as determined by the Mean Decrease Gini. Variables that result in nodes with higher purity have a higher decrease in the Gini coefficient. The Variable Importance plot can also be used for feature selection. Note the break after *Hist_1_180_2_StdDev*. *Hist_2_150_2_Mean* and the variables below it are not contributing as much to the model, so it is possible that removing them would not have a huge impact on the model's performance.

```{r rf_varimp, echo = FALSE, out.width = '350px', fig.cap = 'RF Variable Importance', fig.align = "center", fig.pos = 'H'}

# Variable importance plot
imp <- as.data.frame(rf.model.md$finalModel$importance)
ggplot(imp, aes(x = reorder(rownames(imp), MeanDecreaseGini), MeanDecreaseGini)) +
        geom_bar(stat = "identity", aes(fill = rownames(imp))) + 
        coord_flip() + 
        guides(fill=FALSE) + 
        ggtitle("Random Forest - Variable Importance") +
        xlab("Variables") +
        ylab("Mean Decrease Gini")

```

The confusion matrix for the model is available in __Table 19__. The model misclassified 123 patients as disease free when Pneumoconiosis actually was present. However, the model was able to mimimize the number of false positives and classified 37 patients as having Pneumoconiosis, when the disease was not actually present.

```{r rf_table, echo = FALSE}

kable(rf.cm.md$table, caption = "RF Model Confusion Matrix", booktabs = TRUE) %>% 
        kable_styling(font_size = 8, latex_options = c("striped", "hold_position"))

rf.ROC <- roc(rf.model.md$pred$obs, rf.model.md$pred$Normal)
rf.auc <- auc(rf.ROC)

```

As previosly noted, the AUC in __Figure 16__ of `r rf.auc` is slightly misleading. While the specificity is quite good, the recall still shows some issues. If one were to evaluate the model based soley on the AUC, they might believe the model is performing much better than it actually is.

```{r rf_roc, , echo = FALSE, out.width = '350px', fig.cap = 'RF ROC Curve', fig.align = "center", fig.pos = 'H'}

plot(rf.ROC, col = 'blue', main = paste('RF - Area under the curve (AUC):', round(auc(rf.ROC),2)))

```