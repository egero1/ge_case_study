\subsection{k-Nearest Neighbor} 

The k-Nearest Neighbor [kNN] algorithm is one of the simplest, yet most popular classifiers. It is non-parametric, meaning it makes no assumptions about the distribution of the data. It is also a lazy algorithm, meaning the training phase is very minimal. The algorithm performs classification through a majority vote of its k nearest neighbors, hence the name [@bronshtein_2017]

Some of the advantages of kNN include insensitivity to outliers, high accuracy, and useful for classification and regression. However, kNN has high memory requirements and the data should be centered and scale prior to fitting the model.

```{r knn_model, echo = FALSE}

# Create a data frame to hold the results
knn_results <- data.frame(Model = character()
                          ,Data = character()
                          ,Accuracy = numeric()
                          ,Sensitivity = numeric()
                          ,Specificity = numeric()
                          ,Precision = numeric()
                          ,Kappa = numeric()
                          ,F1 = numeric()
                          ,ROC = numeric())
```

```{r knn_models, echo = FALSE}

# Enable parallel processing and reserve resources
#cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
#registerDoParallel(cluster)

# Set up training conditions - must use LOOCV
fitControl <- trainControl(method = "LOOCV"
                           ,classProbs = TRUE 
                           ,summaryFunction = twoClassSummary
                           ,savePredictions = 'final')

# Full data set
#set.seed(1234)
#knn.model.ud <- caret::train(Label ~ .
#                          ,data = use_data
#                          ,method = "knn"
#                          ,trControl = fitControl
#                          ,metric = "ROC" 
#                          ,preProc = c("center", "scale")
#                          ,tuneLength = 20)

knn.model.ud <- readRDS('../../Models/knn_use_data.rds')

# Get the confusion matrix
knn.cm.ud <- caret::confusionMatrix(knn.model.ud$pred$pred, knn.model.ud$pred$obs, mode = "everything")

# Get the performance metrics from the model and save for comparison
performance <- getTrainPerf(knn.model.ud)
knn_results <- rbind(knn_results
                        ,data.frame(Model = 'kNN'
                        ,Data = 'Full'
                        ,Accuracy = knn.cm.ud$overall[1]
                        ,Sensitivity = performance[,2]
                        ,Specificity = performance[,3]
                        ,Precision = knn.cm.ud$byClass[5]
                        ,Kappa = knn.cm.ud$overall[2]
                        ,F1 = knn.cm.ud$byClass[7]
                        ,ROC = performance[,1]))

# RFE selection data set
#set.seed(1234)
#knn.model.md <- caret::train(Label ~ .
#                             ,data = model_data
#                             ,method = "knn"
#                             ,trControl = fitControl
#                             ,metric = "ROC" 
#                             ,preProc = c("center", "scale")
#                             ,tuneLength = 20)

knn.model.md <- readRDS('../../Models/knn_model_data.rds')

# Get the confusion matrix
knn.cm.md <- caret::confusionMatrix(knn.model.md$pred$pred, knn.model.md$pred$obs, mode = "everything")

# Get the performance metrics from the model and save for comparison
performance <- getTrainPerf(knn.model.md)
knn_results <- rbind(knn_results
                        ,data.frame(Model = 'kNN'
                        ,Data = 'RFE Selection'
                        ,Accuracy = knn.cm.md$overall[1]
                        ,Sensitivity = performance[,2]
                        ,Specificity = performance[,3]
                        ,Precision = knn.cm.md$byClass[5]
                        ,Kappa = knn.cm.md$overall[2]
                        ,F1 = knn.cm.md$byClass[7]
                        ,ROC = performance[,1]))

# Low correlation data set
#set.seed(1234)
#knn.model.lc <- caret::train(Label ~ .
#                             ,data = use_data_lc
#                             ,method = "knn"
#                             ,trControl = fitControl
#                             ,metric = "ROC" 
#                             ,preProc = c("center", "scale")
#                             ,tuneLength = 20)

knn.model.lc <- readRDS('../../Models/knn_use_data_lc.rds')

# Get the confusion matrix
knn.cm.lc <- caret::confusionMatrix(knn.model.lc$pred$pred, knn.model.lc$pred$obs, mode = "everything")

# Get the performance metrics from the model and save for comparison
performance <- getTrainPerf(knn.model.lc)
knn_results <- rbind(knn_results
                        ,data.frame(Model = 'kNN'
                        ,Data = 'Low Correlation'
                        ,Accuracy = knn.cm.lc$overall[1]
                        ,Sensitivity = performance[,2]
                        ,Specificity = performance[,3]
                        ,Precision = knn.cm.lc$byClass[5]
                        ,Kappa = knn.cm.lc$overall[2]
                        ,F1 = knn.cm.lc$byClass[7]
                        ,ROC = performance[,1]))

rownames(knn_results) <- NULL

# Disable parallel processing and release resources
#stopCluster(cluster)
#registerDoSEQ()

```

The evaluation metrics for the kNN models are available in __Table 13__. The trimmed data set from the 'rfe' method performed best with an accuracy of `r round(knn.cm.md$overall[1] * 100, 2)`%. This is the best performing model so far.

```{r knn_results, echo = FALSE}

kable(knn_results, caption = "kNN Model Results", booktabs = TRUE) %>% 
        kable_styling(font_size = 8, latex_options = c("striped", "hold_position"))

```

\FloatBarrier

The confusion matrix for this model is available in __Table 14__. The model predicts Pneumoconiosis in 76 patients when no disease is actually present, but fails to predict the presence of Pneumoconiosis in 95 patients.

```{r knn_table, echo = FALSE}

kable(knn.cm.md$table, caption = "kNN Model Confusion Matrix", booktabs = TRUE) %>% 
        kable_styling(font_size = 8, latex_options = c("striped", "hold_position"))

knn.ROC <- roc(knn.model.md$pred$obs, knn.model.md$pred$Normal)
knn.auc <- auc(knn.ROC)

```

The AUC in __Figure 12__ of `r knn.auc` shows that the model is performing quite well. Recall that the AUC of a perfect model is 1.00.

```{r knn_roc, echo = FALSE, out.width = '350px', fig.cap = 'kNN ROC Curve', fig.align = "center", fig.pos = 'H'}

plot(knn.ROC, col = 'blue', main = paste('kNN - Area under the curve (AUC):', round(auc(knn.ROC),2)))

```

The plot in __Figure 13__ shows how the AUC changes as more neighbors are selected. The model performs best with `r knn.model.md$bestTune` neighbors and the performance degrades as more neighbors are added.

```{r knn_neighbors, echo = FALSE, out.width = '350px', fig.cap = 'Values of k', fig.align = "center", fig.pos = 'H'}

plot(knn.model.md, main = paste('k = ', knn.model.md$bestTune))

```