# Exploratory Data Analysis

Note in __Table 1__ that the count of observations by segment differs. This indicates that not all of the `r length(unique(full$PatientNumMasked))` unique patients have observations for all lung segments. Summary statistics, including the mean, median, standard deviation, mimimum values, maximum values and any missing data were reviewed. They are not included here due to the size of the table, but are availabe in **Appendix A.1**.

```{r observations, echo = FALSE}

observations <- use_data %>% 
        group_by(Position) %>%
        summarize(Count = n())

observations$Position <- as.character(observations$Position)
observations <- rbind(observations, c('Total', sum(observations$Count)))
kable(observations, caption = "Lung Segment Observations", booktabs = TRUE) %>% 
        kable_styling(font_size = 8,  latex_options = c("striped", "hold_position"))

```

```{r check_labels, echo = FALSE}

# Verify that all labels for patients are the same
checklabel <- full %>%
        group_by(PatientNumMasked) %>%
        mutate(minLabel = min(Label)) %>%
        mutate(maxLabel = max(Label)) %>%
        mutate(count = length(PatientNumMasked)) %>%
        dplyr::select(PatientNumMasked, minLabel, maxLabel, count) 

# If the min and max are equal, they will be dropped.  If list is empty
# then the label is consistent across data sets for all patients
checklabel <- checklabel %>%
        dplyr::filter(minLabel != maxLabel)

checklabel <- ifelse(dim(checklabel)[1] == 0, 'no', dim(checklabel)[1])

```

Since the lung images from each patient are broken into six segments and each tab contains the patient number and the label, it is possible that the labels for a patient do not match across the six segments. This is not much of an issue at the segment level, but it is important that it is taken into consideration if the data is rolled up or combined into a single table. If the label for any segement is 'Abnormal', then it should be considered that disease is present in the patient, regardless of the value of the other segment labels. Verification was performed to ensure that `r checklabel` patients have inconsistent label values. The categorical variable, _Position_, indicating the which lung segment the observations belong to was created and added to the data set for use in EDA and modeling.

The prevalence rate for Pneumoconiosis in the data set is roughly 36%. The remaining 64% of patients are labeled as 'Normal', meaning no disease was detected during the initial analysis by the medical experts. 

\subsection{Correlation - Predictors} 
The correlation plot in __Figure 2__ shows that some of the predictors are highly correlated with each other. This requires further investigation as high correlation among predictors can cause instability in the model. Generally, when two variables are highly correlated, it is best to remove one since the second is not providing new information.

```{r correlation, echo = FALSE, out.width = '450px', fig.cap = 'Correlation Matrix', fig.align = "center", fig.pos = 'H'}

# Correlation between predictors
corr <- cor(use_data[-c(40,41)])
corrplot(corr, type = "upper", tl.cex = 0.5, tl.col = "blue", tl.srt = 45)

```

```{r high_correlation, echo = FALSE}

# Review and remove highly correlated predictors
correlationMatrix <- cor(use_data[-c(40,41)])
highCorrelation <- findCorrelation(correlationMatrix, cutoff = 0.50)
rows <- length(colnames(use_data[highCorrelation]))
highly_correlated <- data.frame("Correlated Variables 1" = colnames(use_data[highCorrelation])[1:(rows / 2)], 
                                "Correlated Variables 2" = colnames(use_data[highCorrelation])[(rows/2 + 1):rows])

kable(highly_correlated, caption = "Highly Correlated Variables", booktabs = TRUE, col.names = c('', '')) %>% 
        kable_styling(font_size = 8, latex_options = c("striped", "hold_position"))

```

In __Figure 3__, the variables having a pair-wise correlation greater than 0.5 or less than -0.5 have been removed, see __Table 2__. Since some modeling methods are more sensitive to multicollinearity than others, a new data set will be created from the remaining variables. This reduces the count of predictors from 40 to 12. 

```{r new_correlation, echo = FALSE, out.width = '350px', fig.cap = 'Correlation Matrix - Highly Correlated Variables Removed', fig.align = "center", fig.pos = 'H'}
use_data_lc <- use_data[,-highCorrelation]
low_corr <- use_data_lc %>%
        select(-Position, -Label)
low_corr<- cor(low_corr)

col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(low_corr, method = "color", col = col(200),
         type = "upper", order = "hclust", number.cex = .7,
         addCoef.col = "black", # Add coefficient of correlation
         tl.col = "blue", tl.srt = 45, tl.cex = 0.5, # Text label color and rotation
         # Combine with significance
         sig.level = 0.01, insig = "blank", 
         # hide correlation coefficient on the principal diagonal
         diag = FALSE)

```

\subsection{Distribution - Predictors}

__Figure 4__ provides examples of histograms for several of the predictors. Different modeling methods make assumptions about the distributions of the independent variables which could impact their relationship with the dependent variable, so it is good to review them. The distribution patterns seen here are common among the predictors in the dataset. Histograms for each of the predictors are available in **Appendix A.2**.

```{r distribution, echo = FALSE, out.width = '400px', fig.cap = 'Histograms', fig.align = "center", fig.pos = 'H'}

par(mfrow = c(2,2))
for (i in 1:dim(use_data[c(4, 5, 8, 24)])[2]) {
        hist(use_data[,i], main = colnames(use_data)[i], xlab = "")
}

```

\subsection{Outliers} 

An outlier is an observation that is much smaller or larger than other values. It is important to identify potential outliers since they can skew or add bias to a model. The box plots __Figure 5__ provide an easy way to determine if outliers are present for a variable. The box plots show the minimum and maximum values on the vertical line, and the box which are the boundaries for the 1st and 3rd quartiles. The horizontal line in the middle of the box is the median. Note that in the box plot for *Hist_0_0_0_Kurtosis*, the median is very close to the 1st and 3rd quartiles. Box plots for all predictors were created and reviewed, but were not provided to conserve space. Most variables display patterns similar to those seen in __Figure 5__. Box plots for the remaining variables are provided in **Appendix A.3**.  

The potential outliers are the individual data points that fall above or below the box, or 1st and 3rd quartiles. While these points can be influential, their full impact is difficult to determine in advance. The traditional processes for handling outliers includes removing them or capping their values. Since no information was provided on the valid range of each variable, it will be assumed that all data points are valid. Additionally, some modeling methods are less sensitive to outliers than others, so the potential impact of these points will be consisdered for each model.

```{r boxplots, echo = FALSE, out.width = '350px', fig.cap = 'Box plots', fig.align = "center", fig.pos = 'H'}

# Check for outliers
plots <- list(0)
for(var in 3:4) {
        p <- ggplot(data = use_data, aes(x = '', y = use_data[,var])) + 
                geom_boxplot(color='red', fill='orange', alpha = 0.2) +
                labs(title = paste('Outliers - ', colnames(use_data[var]))) +
                ylab('Boxplot') +
                xlab('') + 
        coord_cartesian(ylim = c(min(use_data[,var]), max(use_data[,var])))
        plots[[var-2]] <- p
}

grid.arrange(plots[[1]], plots[[2]], ncol = 2)

```

\subsection{Evaluation Metrics}

In order to understand how well the models are performing, or detecting Pneumoconiosis, various evaluation metrics will be considered. The most common evaluation metrics for classification include recall (sensitivity), specificity, precision, F1 and accuracy and are based on the rates of True Positive [TP], True Negative [TN], False Positive [FN] and False Negative [FN]. 

Since the evaluation metrcis are defined in terms of TP, TN, FP and FN rates, definitions for each are provided. TP is when the model predicts 'Abnormal' and the patient actually does have Pneumoconiosis. TN is when the model predicts 'Normal' and the patient does not have Pneumoconiosis. FP is when the model predicts 'Abnormal, but the patient does not actually have Pneumoconiosis, and FN is when the model predicts 'Normal', but Pneumoconiosis is present. The following definitions may now be applied:

### Evaluation metric definitions 
* Accuracy - $(\frac{1}{N})|\left \{ i|y_{i} = \hat{y}_{i} \right \}|$ = (TN + TP)/(TN + TP + FN + FP)
* Recall (sensitivity) - $\frac{|\left \{ i|y_{i} = \hat{y}_{i}, \hat{y}_{i} = 1 \right \}|}{|\left \{ i|y_{i} = 1 \right \}|}$ = TP/(TP + FN)
* Specificity - true negative rate = TN/(TN + FP)
* Precision - $\frac{|\left \{ i|y_{i} = \hat{y}_{i}, \hat{y}_{i} = 1 \right \}|}{|\left \{ i|\hat{y}_{i} = 1 \right \}|}$ = TP/(TP + FP)
* F1 - weighted average of precision and recall = 2 * (TP/(TP + FN) * TP/(TP + FP))/(TP/(TP + FN) + TP/(TP + FP))
* Kappa - (observed accuracy - expected accuracy)/(1 - expected accuracy) 
    + observed accuracy = (TN + TP) / (TN + TP + FN + FP)
    + expected accuracy = (((TN + FP) * (TN + FN)) / (TN + TP + FN + FP) + (TN * (FP + TP))  / (TN + TP + FN + FP)) / (TN + TP + FN + FP)

F1 takes both false positives and false negatives into account, so it can be more useful than accuracy alone, especially when there is an uneven class distribution[@evalmetric]. The Kappa statistic will also be used for model evaluation and comparison. The Kappa statistic compares the observed accuracy with the expected accuracy and takes random chance into account [@82187].

\subsection{Naive Model}

A simple decision tree was used to fit a naive model. This model uses the full data set containing all precitors and with no transformations or scaling. The purpose of this model is to provide a baseline for the models that will be built since any improvements made to the data set or modeling method should result in a better performing model. __Table 3__ provides the simple confusion matrix for the model. The model predicted 'Abnormal' for 750 observations when the true result actually was 'Abnormal', and predicted 'Normal' for 1,582 observations when the true result actually was 'Normal'. However, the model incorrectly classified 194 observations as 'Abnormal' when the true result was actually 'Normal', and 80 observations as 'Abnormal' when the true result was 'Normal'.

```{r naive_cm, echo = FALSE, , comment = NA}
tree <- rpart(Label ~., data = use_data)

tree.pred <- predict(tree, use_data, type = 'class')
tree.cm <- confusionMatrix(tree.pred, use_data$Label, positive = 'Abnormal')

kable(tree.cm$table, caption = "Confusion Matrix", booktabs = TRUE) %>% 
        kable_styling(font_size = 8, latex_options = c("striped", "hold_position"))
```

The evaluation metrics for the naive model are provided in __Table 4__. The performance of this simple model is much better than expected, with an accuracy of 89% and an F1 of 85%. With a specificity of 95% and a recall of 79%, the model is better at identifying true negatives than it is at identifying true positives. Note that precision, also known as the positive predictive value, is 90%.

```{r naive_cm1, echo = FALSE, , comment = NA}
tree.combined <- data.frame(c(tree.cm$overall, tree.cm$byClass))

kable(tree.combined, caption = "Evaluation Metrics", booktabs = TRUE, col.names = 'Naive Model Metrics') %>% 
        kable_styling(font_size = 8, latex_options = c("striped", "hold_position"))

```

The naive model is also useful for identifying the variables that are likely to be the best predictors. As seen in __Figure 6__, the variables _Hist_2_150_2_Entropy_, _Hist_2_30_2_Entropy_, and _Hist_1_180_2_Mean_ play key roles in this model. Any observation having a _Hist_2_150_2_Entropy_ greater than or equal to 2.7 are then evaluated against _Hist_2_30_2_Entropy_, while observations having a _Hist_2_150_2_Entropy_ of less than 2.7 are evaluated against _Hist_1_180_2_Mean_. 

```{r naive_model, echo = FALSE, out.width = '350px', fig.cap = 'Naive Decision Tree', fig.align = "center", fig.pos = 'H'}

rpart.plot(tree)

```

__Figure 7__ provides the Receiver Operating Characteristic curve (ROC). The ROC curve is a useful tool for evaluating predictive models and shows how well the model can distinguish between the true positives and the true negatives. The curve is the result of plotting the recall against 1 - specificty. The further the curve is from the diagonal line, the better the model is at discriminating between positives and negatives. A ROC curve that hugs the diagonal line means the model's predictive capabilities are not much better than chance.

Another useful statistic is the Area Under the Curve (AUC). This metric also measures how well the model predicts. An AUC of '1' indicates that the model perfectly separates the true positives from the true negatives, while an AUC of '0.5' means the model is not able to separate the true positives from the true negatives. ROC curves and AUC measures will also be used to evaluate the fitted models.

```{r naive_roc, echo = FALSE, out.width = '350px', fig.cap = 'Naive ROC Curve', fig.align = "center", fig.pos = 'H'}
tree.pred <- predict(tree, use_data)
tree.ROC <- roc(use_data$Label, tree.pred[,1])
plot(tree.ROC, col = 'blue', main = paste('Area under the curve (AUC):', round(auc(tree.ROC),2)))

```

\subsection{Variable Importance}

It is always beneficial to reduce the complexity and dimensionality of a model whenever possible. One way to simplify a model is to reduce the number of predictors to include only those that actually provide value for the model. Including more predictors than needed increases the run-time for the model, and makes it prone to overfitting. An overfit model is one that fits the training data too well because it has learned the details and the noise. While the model may provide excellent results on the training data, it typically performs poorly on new data.

The 'caret' package in R provides the 'rfe' function for recursive feature elimination which is a simple backwards selection process to identify the best predictors. The function implements backwards selection of predictors based on predictor importance ranking. The predictors are ranked and the less important ones are sequentially eliminated prior to modeling [@kuhn]. The goal is to find a subset of predictors that can be used to produce an accurate model. __Figure 8__ shows that the best accuracy - roughly 93%, is achieved with 16 variables. Note that the accuracy degrades as more predictors are added to the model. 

It is also worth noting that the 'rfe' function allows the selection process to be performed using numerous cross validation techniques, including leave one out cross validation. However, the function was implemented using 10-fold cross validation, repeated five times. The results were almost identical to those of LOOCV but completed in seconds while the LOOCV method took over an hour to run.

```{r model_data, include = FALSE}

# Enable parallel processing and reserve resources
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(1234)
control <- rfeControl(functions = rfFuncs
                      ,method = "repeatedcv"
                      ,number = 5
                      ,verbose = FALSE
                      ,allowParallel = TRUE)

pref_variables <- rfe(use_data[-40], use_data[,40], rfeControl = control)

# Print the results
#pref_variables

# List the variables
#predictors(pref_variables)

# Disable parallel processing and release resources
stopCluster(cluster)
registerDoSEQ()

```

```{r var_plot, echo = FALSE, out.width = '350px', fig.cap = 'Variable Selection', fig.align = "center", fig.pos = 'H'}

plot(pref_variables, type = c("g", "o"))

# Create a data set using the preferred variables
model_data <- use_data %>%
        dplyr::select(predictors(pref_variables), Label)

```

The predictors selected through the 'rfe' function are listed in __Table 5__. This list includes all but three of the variables selected in the naive model. The predictors selected by removing the highly correlated varaiables are listed in __Table 6__. This list of predictors is quite different from those selected in the naive model.

```{r selection, echo = FALSE, comment = NA}

# Compare the two data sets
predictors_md <- as.data.frame(colnames(model_data[-17]))
predictors_lc <- as.data.frame(colnames(use_data_lc[-12]))
colnames(predictors_md) <- "Backwards Selection"
colnames(predictors_lc) <- "Low Correlation"
common_predictors <- as.data.frame(intersect(predictors_md$`Backwards Selection`, predictors_lc$`Low Correlation`))
colnames(common_predictors) <- "Common Predictors"

kable(predictors_md, caption = "Backwards Selection Variables", booktabs = TRUE) %>% 
        kable_styling(font_size = 8, latex_options = c("striped", "hold_position"))

kable(predictors_lc, caption = "Low Correlation Predictor Variables", booktabs = TRUE) %>% 
        kable_styling(font_size = 8, latex_options = c("striped", "hold_position"))
```

\FloatBarrier

__Table 7__ lists the six variables that are common for both selection sets. It would preferrable if more variables were common between the two data sets, but it is not too surprising that the lists differ. Simply removing the variables that are highly correlated does not necessarily mean that the remaining varaibles are good predictors. Models will be fitted using the full data set, the backwards selection data set and the low correlation data set so that the results may be compared.

```{r common, echo = FALSE, comment = NA}
kable(common_predictors, caption = "Common Predictor Variables", booktabs = TRUE) %>% 
        kable_styling(font_size = 8, latex_options = c("striped", "hold_position"))


```

Since only the low correlation data set included the _Position_ variable, the six lung segments will be modeled as one data set. This prevents the need of having to run mulitiple models of each type for each lung segment. While it is possible that the data from a particular segment may be more predictive than others, this could vary by model type. If performance from the combined data sets is less than adequate then further analysis may be necessary on the individual segments to understand if there is a statistically significant difference between them.